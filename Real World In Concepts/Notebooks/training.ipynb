{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODvMe7eNGRo+RCtBMsCDXK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gracialukelo/deep_ann/blob/main/training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIS3W0Nyz81W"
      },
      "outputs": [],
      "source": [
        "# Gradientproblme\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradientenprobleme:\n",
        "\n",
        "1. Schwindende Gradienten: Wenn die Gradienten beim Rückwärtsdurchlauf durch das Netzwerk immer kleiner werden, können die frühen Schichten im Netzwerk kaum lernen.\n",
        "\n",
        "2. Explodierende Gradienten: Umgekehrt können die Gradienten auch sehr groß werden, was zu instabilem Lernen führt.\n",
        "\n",
        "Beispiel: Stellen Sie sich vor, Sie versuchen, durch einen dichten Nebel zu navigieren. Wenn der Nebel immer dichter wird (schwindende Gradienten), wird es schwer, den Weg zu finden. Wenn der Nebel plötzlich stark auftritt (explodierende Gradienten), kann man den Überblick verlieren."
      ],
      "metadata": {
        "id": "8r23QuPw0Uo2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lösungen:\n",
        "\n",
        "Normalisierungstechniken: Batch-Normalisierung kann helfen, die Gradienten in einem stabilen Bereich zu halten.\n",
        "\n",
        "Initialisierung der Gewichte: Eine gute Initialisierung der Netzwerkgewichte kann verhindern, dass die Gradienten explodieren oder verschwinden."
      ],
      "metadata": {
        "id": "GZgatYuB0l0u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cHPsDGJq0isW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fehlende Trainingsdaten:\n",
        "\n",
        "Große Netze benötigen eine Menge an Trainingsdaten, die oft schwer zu beschaffen sind oder teuer zu labeln sind.\n",
        "\n",
        "Beispiel: Wenn Sie ein Kind mit sehr vielen unterschiedlichen Spielzeugen spielen lassen wollen, aber nur eine kleine Anzahl von Spielzeugen haben, wird es schwer sein, alle Spielzeuge zu erkennen.\n",
        "\n",
        "\n",
        "Lösungen:\n",
        "\n",
        "Transfer Learning: Ein vortrainiertes Modell wird verwendet, um von bereits vorhandenen Daten zu lernen und dann für spezifische Aufgaben angepasst.\n",
        "\n",
        "Unüberwachtes Pretraining: Vortraining des Modells auf unbeschrifteten Daten, um ein gutes Startmodell zu erhalten."
      ],
      "metadata": {
        "id": "0-mW4BV407ZP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Langsame Trainingszeiten:\n",
        "\n",
        "Das Training von großen Netzen kann sehr zeitaufwendig sein, da die Modelle viele Parameter und Berechnungen erfordern.\n",
        "\n",
        "Beispiel: Das Backen eines großen Kuchens dauert länger als das eines kleinen, da mehr Zutaten und Zeit benötigt werden.\n",
        "\n",
        "\n",
        "Lösungen:\n",
        "\n",
        "Optimierer: Einsatz effizienter Optimierungsalgorithmen wie Adam oder RMSprop, die schneller konvergieren als einfache Methoden wie Stochastic Gradient Descent.\n",
        "\n",
        "\n",
        "\n",
        "## Overfitting:\n",
        "\n",
        "Ein Modell mit vielen Parametern kann die Trainingsdaten zu genau lernen und dabei die Fähigkeit verlieren, auf neuen, unbekannten Daten gut zu performen.\n",
        "\n",
        "Beispiel: Ein Schüler, der nur die Fragen der letzten Prüfung auswendig lernt, kann bei neuen Fragen Schwierigkeiten haben.\n",
        "\n",
        "\n",
        "Lösungen:\n",
        "\n",
        "Regularisierungstechniken: Methoden wie Dropout oder L2-Regularisierung verhindern, dass das Modell zu stark an den Trainingsdaten haftet und verbessern die Generalisierungsfähigkeit des Modells.\n"
      ],
      "metadata": {
        "id": "xtUkO8Xt1MEm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Zusammenfassung und Implementierung:\n",
        "\n",
        "**Gradientenprobleme lösen:** Verwenden Sie Batch-Normalisierung und gute Gewichtinitialisierung.\n",
        "\n",
        "**Fehlende Trainingsdaten:** Nutzen Sie Transfer Learning und unüberwachtes Pretraining.\n",
        "\n",
        "**Trainingszeiten reduzieren:** Setzen Sie moderne Optimierer wie Adam ein.\n",
        "\n",
        "**Overfitting vermeiden:** Wenden Sie Regularisierungstechniken wie Dropout an."
      ],
      "metadata": {
        "id": "A8lRO9n92j3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Batch-Normalisierung: Erklärung und mathematische Berechnung\n",
        "Ziel: Werte eines neuronalen Netzes während des Trainings stabilisieren, indem die Inputs jeder Schicht normalisiert werden.\n",
        "\n",
        "Batch-Normalisierung ist eine Technik, die verwendet wird, um die Eingaben einer Schicht in einem neuronalen Netzwerk während des Trainings zu stabilisieren. Hier ist die mathematische Beschreibung der Schritte:\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Zentrierung und Normalisierung\n",
        "\n",
        "In jedem Mini-Batch werden die Eingaben so transformiert, dass sie einen Mittelwert von **0** und eine Standardabweichung von **1** haben. Dies wird wie folgt berechnet:\n",
        "\n",
        "1. **Berechnung des Mittelwerts (\\( \\mu_B \\))** für jede Eingabe in dem aktuellen Batch:\n",
        "   $$\n",
        "   \\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i\n",
        "   $$\n",
        "\n",
        "2. **Berechnung der Varianz (\\( \\sigma_B^2 \\))**:\n",
        "   $$\n",
        "   \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^m (x_i - \\mu_B)^2\n",
        "   $$\n",
        "\n",
        "3. **Normalisierung der Eingaben (\\( \\hat{x}_i \\))**:\n",
        "   $$\n",
        "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "   $$\n",
        "   Hier ist \\( \\epsilon \\) eine kleine Konstante, um Division durch Null zu vermeiden.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Skalierung und Verschiebung\n",
        "\n",
        "Nach der Normalisierung werden die Eingaben mittels zwei trainierbarer Parameter transformiert:\n",
        "\n",
        "1. **Skalierung mit \\( \\gamma \\)** (wird während des Trainings gelernt):\n",
        "   $$\n",
        "   y_i = \\gamma \\hat{x}_i\n",
        "   $$\n",
        "\n",
        "2. **Verschiebung mit \\( \\beta \\)** (wird ebenfalls während des Trainings gelernt):\n",
        "   $$\n",
        "   y_i = \\gamma \\hat{x}_i + \\beta\n",
        "   $$\n",
        "\n",
        "Diese Transformation erlaubt es dem Netzwerk, die optimalen Werte für die Eingaben jeder Schicht zu lernen.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3tLTKAp5j0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Python\n",
        "import numpy as np\n",
        "\n",
        "# Eingaben in einem Batch\n",
        "x = np.array([10, 20, 30, 40], dtype=float)\n",
        "\n",
        "# Schritt 1: Berechnung von Mittelwert und Standardabweichung\n",
        "mu_B = np.mean(x)  # Mittelwert\n",
        "sigma_B = np.std(x)  # Standardabweichung\n",
        "\n",
        "print(\"Mittelwert (μ_B):\", mu_B)\n",
        "print(\"Standardabweichung (σ_B):\", sigma_B)\n",
        "\n",
        "# Schritt 2: Normalisierung\n",
        "x_hat = (x - mu_B) / sigma_B\n",
        "\n",
        "print(\"\\nNormalisierte Werte (x^):\", x_hat)\n",
        "\n",
        "# Schritt 3: Skalierung und Verschiebung\n",
        "gamma = 2  # Skalierungsparameter\n",
        "beta = 1   # Verschiebungsparameter\n",
        "\n",
        "y = gamma * x_hat + beta\n",
        "\n",
        "print(\"\\nErgebnis nach Skalierung und Verschiebung (y):\", y)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-NLvwaW-1Ll",
        "outputId": "0b540948-ccce-422c-87e0-056f01bd308c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mittelwert (μ_B): 25.0\n",
            "Standardabweichung (σ_B): 11.180339887498949\n",
            "\n",
            "Normalisierte Werte (x^): [-1.34164079 -0.4472136   0.4472136   1.34164079]\n",
            "\n",
            "Ergebnis nach Skalierung und Verschiebung (y): [-1.68328157  0.10557281  1.89442719  3.68328157]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Beispiel-Daten: Mini-Batch mit 4 Samples, 3 Features pro Sample\n",
        "inputs = torch.tensor([\n",
        "    [10.0, 20.0, 30.0],\n",
        "    [15.0, 25.0, 35.0],\n",
        "    [20.0, 30.0, 40.0],\n",
        "    [25.0, 35.0, 45.0]\n",
        "])\n",
        "\n",
        "# Batch-Normalisierungsschicht\n",
        "# num_features: Anzahl der Eingabefeatures (hier: 3)\n",
        "batch_norm = nn.BatchNorm1d(num_features=3)\n",
        "\n",
        "# Die Schicht ist standardmäßig trainierbar, wir verwenden jedoch hier Beispiel-Daten\n",
        "# Forward Pass durch die Batch-Normalisierungsschicht\n",
        "normalized_inputs = batch_norm(inputs)\n",
        "\n",
        "print(\"Eingaben:\")\n",
        "print(inputs)\n",
        "print(\"\\nNormalisierte Eingaben:\")\n",
        "print(normalized_inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNP6KncB0WyK",
        "outputId": "6532a42e-05a6-43a2-fb61-39d58036d282"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eingaben:\n",
            "tensor([[10., 20., 30.],\n",
            "        [15., 25., 35.],\n",
            "        [20., 30., 40.],\n",
            "        [25., 35., 45.]])\n",
            "\n",
            "Normalisierte Eingaben:\n",
            "tensor([[-1.3416, -1.3416, -1.3416],\n",
            "        [-0.4472, -0.4472, -0.4472],\n",
            "        [ 0.4472,  0.4472,  0.4472],\n",
            "        [ 1.3416,  1.3416,  1.3416]], grad_fn=<NativeBatchNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient Clipping\n",
        "\n",
        "Gradient Clipping ist eine direkte Technik, um exploding gradients zu verhindern. Es greift ein, wenn die Gradienten während des Backpropagationsschritts zu groß werden.\n",
        "\n",
        "\n",
        "\n",
        "#### Clipping nach Wert (Value Clipping)\n",
        "Jeder Wert im Gradienten wird auf das Intervall\n",
        "[\n",
        "−\n",
        "1.0\n",
        ",\n",
        "1.0\n",
        "]\n",
        "[−1.0,1.0] begrenzt.\n",
        "Werte, die größer als 1.0 oder kleiner als -1.0 sind, werden auf 1.0 bzw. -1.0 gesetzt.\n",
        "\n",
        "#### Clipping nach Norm (Norm Clipping)\n",
        "Die gesamte Norm der Gradienten (also die Länge des Gradientenvektors im Raum) wird begrenzt.\n",
        "Wenn die Norm der Gradienten größer als max_norm ist, wird der gesamte Vektor skaliert, sodass die Norm\n",
        "≤\n",
        "max_norm\n",
        "≤max_norm ist."
      ],
      "metadata": {
        "id": "LFLbLYraGZQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'import torch\n",
        "\n",
        "#\n",
        "\n",
        "# Beispiel: Gradienten\n",
        "gradients = torch.tensor([0.5, 2.0, -3.0, 4.0], requires_grad=True)\n",
        "\n",
        "# Clipping-Schwellenwert (Maximum für jeden Gradientenwert)\n",
        "clip_value = 1.0\n",
        "\n",
        "# Clipping anwenden\n",
        "clipped_gradients = torch.clamp(gradients, min=-clip_value, max=clip_value)\n",
        "\n",
        "print(\"Originale Gradienten:\", gradients)\n",
        "print(\"Geclippte Gradienten (nach Wert):\", clipped_gradients)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcxomI1n02Oe",
        "outputId": "8e5362bc-fa2b-425b-a4f9-1efd597ac656"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Originale Gradienten: tensor([ 0.5000,  2.0000, -3.0000,  4.0000], requires_grad=True)\n",
            "Geclippte Gradienten (nach Wert): tensor([ 0.5000,  1.0000, -1.0000,  1.0000], grad_fn=<ClampBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel: Gradienten\n",
        "gradients = torch.tensor([0.5, 2.0, -3.0, 4.0], requires_grad=True)\n",
        "\n",
        "# Clipping-Schwellenwert (maximale Norm)\n",
        "max_norm = 2.0\n",
        "\n",
        "# Norm der Gradienten berechnen\n",
        "gradient_norm = torch.norm(gradients)\n",
        "\n",
        "# Clipping anwenden, falls die Norm größer als max_norm ist\n",
        "if gradient_norm > max_norm:\n",
        "    clipped_gradients = gradients * (max_norm / gradient_norm)\n",
        "else:\n",
        "    clipped_gradients = gradients\n",
        "\n",
        "print(\"\\nOriginale Gradienten:\", gradients)\n",
        "print(\"Norm der Gradienten vor Clipping:\", gradient_norm)\n",
        "print(\"Geclippte Gradienten (nach Norm):\", clipped_gradients)\n",
        "print(\"Norm der Gradienten nach Clipping:\", torch.norm(clipped_gradients))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0JIt2zMCJi4",
        "outputId": "cd2025d5-80b8-4b28-a702-1d2eed91df24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Originale Gradienten: tensor([ 0.5000,  2.0000, -3.0000,  4.0000], requires_grad=True)\n",
            "Norm der Gradienten vor Clipping: tensor(5.4083, grad_fn=<LinalgVectorNormBackward0>)\n",
            "Geclippte Gradienten (nach Norm): tensor([ 0.1849,  0.7396, -1.1094,  1.4792], grad_fn=<MulBackward0>)\n",
            "Norm der Gradienten nach Clipping: tensor(2., grad_fn=<LinalgVectorNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MN7usmOFCM_N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}